{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13943f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Improved AQI Prediction Model Training\n",
    "Uses advanced feature engineering, XGBoost/LightGBM, and LSTM/GRU for better predictions\n",
    "\"\"\"\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Set UTF-8 encoding for Windows console\n",
    "if sys.platform == 'win32':\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')\n",
    "    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import advanced models\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"️ XGBoost not available, will use LightGBM or GradientBoosting\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"️ LightGBM not available, will use GradientBoosting\")\n",
    "\n",
    "# Try to import TensorFlow/Keras for LSTM/GRU\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "    # Set random seed for reproducibility\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "except ImportError:\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "    print(\"️ TensorFlow not available, LSTM/GRU models will be skipped\")\n",
    "    print(\"   Install with: pip install tensorflow\")\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "try:\n",
    "    from backend.hopsworks_utils import get_offline_features, load_from_csv_fallback\n",
    "    USE_HOPSWORKS = True\n",
    "except ImportError:\n",
    "    USE_HOPSWORKS = False\n",
    "    print(\"️ Hopsworks not available, will load from CSV\")\n",
    "\n",
    "def augment_dataset(df, augmentation_factor=20):\n",
    "    \"\"\"\n",
    "    MASSIVELY augment dataset using multiple sophisticated techniques\n",
    "    This creates an enormous dataset for better deep learning model training\n",
    "    \"\"\"\n",
    "    print(f\"\\n MASSIVE Dataset Augmentation (target: {augmentation_factor}x)...\")\n",
    "    original_size = len(df)\n",
    "    \n",
    "    augmented_dfs = [df.copy()]\n",
    "    \n",
    "    # Technique 1: Gaussian noise with varying intensities\n",
    "    print(\"   Technique 1: Gaussian noise variations...\")\n",
    "    for i in range(int(augmentation_factor * 0.3)):\n",
    "        aug_df = df.copy()\n",
    "        numeric_cols = aug_df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if col not in ['year', 'month', 'day', 'hour', 'index']:\n",
    "                noise_factor = np.random.uniform(0.005, 0.08)\n",
    "                noise = np.random.normal(0, aug_df[col].std() * noise_factor, len(aug_df))\n",
    "                aug_df[col] = aug_df[col] + noise\n",
    "                if col in ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Calculated_AQI', 'aqi_index']:\n",
    "                    aug_df[col] = np.maximum(aug_df[col], 0)\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Technique 2: Linear interpolation between consecutive rows\n",
    "    print(\"   Technique 2: Linear interpolation...\")\n",
    "    for i in range(int(augmentation_factor * 0.2)):\n",
    "        aug_df = df.copy()\n",
    "        numeric_cols = aug_df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if col not in ['year', 'month', 'day', 'hour', 'index']:\n",
    "                # Interpolate between consecutive values\n",
    "                alpha = np.random.uniform(0.2, 0.8)\n",
    "                aug_df[col] = aug_df[col] * (1 - alpha) + aug_df[col].shift(1).fillna(aug_df[col]) * alpha\n",
    "                if col in ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Calculated_AQI', 'aqi_index']:\n",
    "                    aug_df[col] = np.maximum(aug_df[col], 0)\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Technique 3: Seasonal pattern variations\n",
    "    print(\"   Technique 3: Seasonal pattern variations...\")\n",
    "    for i in range(int(augmentation_factor * 0.15)):\n",
    "        aug_df = df.copy()\n",
    "        if 'month' in aug_df.columns:\n",
    "            # Add seasonal multipliers\n",
    "            seasonal_mult = 1 + 0.1 * np.sin(2 * np.pi * aug_df['month'] / 12 + np.random.uniform(0, 2*np.pi))\n",
    "            numeric_cols = aug_df.select_dtypes(include=[np.number]).columns\n",
    "            for col in numeric_cols:\n",
    "                if col not in ['year', 'month', 'day', 'hour', 'index']:\n",
    "                    aug_df[col] = aug_df[col] * seasonal_mult\n",
    "                    if col in ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Calculated_AQI', 'aqi_index']:\n",
    "                        aug_df[col] = np.maximum(aug_df[col], 0)\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Technique 4: Time-based shifts with pattern preservation\n",
    "    print(\"   Technique 4: Time-based shifts...\")\n",
    "    for i in range(int(augmentation_factor * 0.15)):\n",
    "        aug_df = df.copy()\n",
    "        if 'datetime' in aug_df.columns:\n",
    "            shift_hours = np.random.choice([-3, -2, -1, 1, 2, 3, 6, 12], size=len(aug_df), \n",
    "                                         p=[0.1, 0.1, 0.15, 0.15, 0.1, 0.1, 0.15, 0.15])\n",
    "            aug_df['datetime'] = pd.to_datetime(aug_df['datetime']) + pd.to_timedelta(shift_hours, unit='h')\n",
    "            # Update time-based columns\n",
    "            if 'hour' in aug_df.columns:\n",
    "                aug_df['hour'] = aug_df['datetime'].dt.hour\n",
    "            if 'day' in aug_df.columns:\n",
    "                aug_df['day'] = aug_df['datetime'].dt.day\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Technique 5: Mix-and-match (combine features from different rows)\n",
    "    print(\"   Technique 5: Feature mixing...\")\n",
    "    for i in range(int(augmentation_factor * 0.1)):\n",
    "        aug_df = df.copy()\n",
    "        # Randomly swap some features between rows\n",
    "        swap_indices = np.random.choice(len(aug_df), size=min(100, len(aug_df)//10), replace=False)\n",
    "        for idx in swap_indices:\n",
    "            swap_with = np.random.choice(len(aug_df))\n",
    "            # Swap weather features but keep pollutants together\n",
    "            weather_cols = [c for c in aug_df.columns if 'temperature' in c or 'humidity' in c or \n",
    "                          'wind' in c or 'precipitation' in c or 'pressure' in c]\n",
    "            for col in weather_cols:\n",
    "                if col in aug_df.columns:\n",
    "                    aug_df.loc[idx, col], aug_df.loc[swap_with, col] = \\\n",
    "                        aug_df.loc[swap_with, col], aug_df.loc[idx, col]\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Technique 6: Polynomial transformations (smooth variations)\n",
    "    print(\"   Technique 6: Polynomial transformations...\")\n",
    "    for i in range(int(augmentation_factor * 0.1)):\n",
    "        aug_df = df.copy()\n",
    "        numeric_cols = aug_df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if col not in ['year', 'month', 'day', 'hour', 'index']:\n",
    "                # Apply smooth polynomial transformation\n",
    "                alpha = np.random.uniform(0.8, 1.2)\n",
    "                beta = np.random.uniform(-0.1, 0.1)\n",
    "                aug_df[col] = alpha * aug_df[col] + beta * (aug_df[col] ** 2) / (aug_df[col].max() + 1e-6)\n",
    "                if col in ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3', 'Calculated_AQI', 'aqi_index']:\n",
    "                    aug_df[col] = np.maximum(aug_df[col], 0)\n",
    "        augmented_dfs.append(aug_df)\n",
    "    \n",
    "    # Combine all augmented data\n",
    "    print(\"   Combining all augmented data...\")\n",
    "    augmented_df = pd.concat(augmented_dfs, ignore_index=True)\n",
    "    \n",
    "    # Shuffle to mix original and augmented data\n",
    "    augmented_df = augmented_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"    Original size: {original_size:,}\")\n",
    "    print(f\"    Augmented size: {len(augmented_df):,}\")\n",
    "    print(f\"    Expansion: {len(augmented_df) / original_size:.2f}x\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "def create_sequences(data, target, sequence_length=24, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM/GRU models\n",
    "    sequence_length: number of time steps to look back\n",
    "    forecast_horizon: number of time steps to predict ahead\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length - forecast_horizon + 1):\n",
    "        X_seq.append(data[i:i+sequence_length])\n",
    "        y_seq.append(target[i+sequence_length:i+sequence_length+forecast_horizon])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IMPROVED AQI MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data\n",
    "if USE_HOPSWORKS:\n",
    "    print(\"\\n Loading data from Hopsworks feature store...\")\n",
    "    end_date = datetime.now()\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    try:\n",
    "        df = get_offline_features(start_date, end_date)\n",
    "        if df is None or df.empty:\n",
    "            USE_HOPSWORKS = False\n",
    "            print(\"️ Hopsworks returned empty data, falling back to CSV\")\n",
    "    except Exception as e:\n",
    "        print(f\"️ Error loading from Hopsworks: {e}, falling back to CSV\")\n",
    "        USE_HOPSWORKS = False\n",
    "\n",
    "if not USE_HOPSWORKS:\n",
    "    print(\"\\n Loading data from CSV...\")\n",
    "    data_path = Path(\"cleaned_aqi_weather_dataset.csv\")\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\" Loaded {len(df)} rows\")\n",
    "\n",
    "# Sort by timestamp if available\n",
    "if 'event_timestamp' in df.columns:\n",
    "    df['event_timestamp'] = pd.to_datetime(df['event_timestamp'])\n",
    "    df = df.sort_values('event_timestamp').reset_index(drop=True)\n",
    "elif 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Create datetime index for time series features\n",
    "if 'event_timestamp' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['event_timestamp'])\n",
    "elif 'timestamp' in df.columns:\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
    "else:\n",
    "    # Create datetime from year, month, day, hour\n",
    "    df['datetime'] = pd.to_datetime(\n",
    "        df[['year', 'month', 'day', 'hour']].apply(\n",
    "            lambda x: f\"{int(x['year'])}-{int(x['month'])}-{int(x['day'])} {int(x['hour'])}:00:00\",\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = df.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Try WITHOUT augmentation first to see baseline performance\n",
    "# Augmentation can introduce noise and distribution shifts\n",
    "USE_AUGMENTATION = False  # Set to True if needed, but start without\n",
    "USE_MINIMAL_FEATURES = True  # Try with only raw pollutants/weather first to diagnose\n",
    "\n",
    "if USE_AUGMENTATION:\n",
    "    print(\"\\n Expanding dataset...\")\n",
    "    df = augment_dataset(df, augmentation_factor=2)  # Minimal augmentation if needed\n",
    "    df = df.sort_values('datetime').reset_index(drop=True)\n",
    "else:\n",
    "    print(\"\\n⏭️  Skipping augmentation - using original data to avoid distribution shifts\")\n",
    "\n",
    "print(\"\\n Feature Engineering...\")\n",
    "\n",
    "# Define pollutant and weather columns\n",
    "pollutant_cols = ['co', 'no', 'no2', 'o3', 'so2', 'pm2_5', 'pm10', 'nh3']\n",
    "weather_cols = [\n",
    "    'temperature_2m', 'relative_humidity_2m', 'precipitation',\n",
    "    'wind_speed_10m', 'wind_direction_10m', 'surface_pressure',\n",
    "    'dew_point_2m', 'apparent_temperature', 'shortwave_radiation',\n",
    "    'et0_fao_evapotranspiration'\n",
    "]\n",
    "\n",
    "# Option to use only raw features for diagnosis\n",
    "if USE_MINIMAL_FEATURES:\n",
    "    print(\"  ️  Using MINIMAL features (raw pollutants + weather only) for diagnosis\")\n",
    "    print(\"  This helps identify if the issue is with feature engineering\")\n",
    "    # Skip all feature engineering - use only raw values\n",
    "    pass\n",
    "else:\n",
    "    # Create lag features (previous hours) - MORE LAGS\n",
    "    # NOTE: Do NOT include target variable in lag features (data leakage!)\n",
    "    print(\"  Creating lag features...\")\n",
    "    for lag in [1, 2, 3, 6, 12, 18, 24, 48]:\n",
    "        for col in pollutant_cols:  # Only use pollutant columns, NOT target\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_lag_{lag}h'] = df[col].shift(lag)\n",
    "\n",
    "    # Create rolling statistics - MORE WINDOWS\n",
    "    print(\"  Creating rolling statistics...\")\n",
    "    for window in [3, 6, 12, 24, 48, 72]:\n",
    "        for col in pollutant_cols:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_rolling_mean_{window}h'] = df[col].rolling(window=window, min_periods=1).mean()\n",
    "                df[f'{col}_rolling_std_{window}h'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n",
    "                df[f'{col}_rolling_min_{window}h'] = df[col].rolling(window=window, min_periods=1).min()\n",
    "                df[f'{col}_rolling_max_{window}h'] = df[col].rolling(window=window, min_periods=1).max()\n",
    "\n",
    "    # Time-based features\n",
    "    print(\"  Creating time-based features...\")\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "    df['day_of_year_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "    df['day_of_year_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "    df['is_weekend'] = (df['datetime'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "    # Weather interactions\n",
    "    print(\"  Creating interaction features...\")\n",
    "    if 'temperature_2m' in df.columns and 'relative_humidity_2m' in df.columns:\n",
    "        df['temp_humidity_interaction'] = df['temperature_2m'] * df['relative_humidity_2m']\n",
    "    if 'wind_speed_10m' in df.columns and 'wind_direction_10m' in df.columns:\n",
    "        df['wind_x'] = df['wind_speed_10m'] * np.cos(np.radians(df['wind_direction_10m']))\n",
    "        df['wind_y'] = df['wind_speed_10m'] * np.sin(np.radians(df['wind_direction_10m']))\n",
    "\n",
    "    # PM ratio features (important for AQI)\n",
    "    if 'pm2_5' in df.columns and 'pm10' in df.columns:\n",
    "        df['pm2_5_pm10_ratio'] = df['pm2_5'] / (df['pm10'] + 1e-6)\n",
    "\n",
    "    # Advanced pollutant interactions\n",
    "    print(\"  Creating advanced interaction features...\")\n",
    "    if 'pm2_5' in df.columns and 'temperature_2m' in df.columns:\n",
    "        df['pm2_5_temp_interaction'] = df['pm2_5'] * df['temperature_2m']\n",
    "    if 'pm10' in df.columns and 'wind_speed_10m' in df.columns:\n",
    "        df['pm10_wind_interaction'] = df['pm10'] * df['wind_speed_10m']\n",
    "    if 'o3' in df.columns and 'temperature_2m' in df.columns:\n",
    "        df['o3_temp_interaction'] = df['o3'] * df['temperature_2m']\n",
    "\n",
    "    # Rate of change features (important for time series)\n",
    "    print(\"  Creating rate of change features...\")\n",
    "    for col in pollutant_cols[:4]:  # Top 4 pollutants\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_rate_change'] = df[col].diff().fillna(0)\n",
    "            df[f'{col}_rate_change_abs'] = df[f'{col}_rate_change'].abs()\n",
    "\n",
    "    # Exponential moving averages (capture trends) - MORE WINDOWS\n",
    "    print(\"  Creating exponential moving averages...\")\n",
    "    for col in ['pm2_5', 'pm10', 'o3', 'no2', 'co']:\n",
    "        if col in df.columns:\n",
    "            for span in [3, 6, 12, 24, 48]:\n",
    "                df[f'{col}_ema_{span}h'] = df[col].ewm(span=span, adjust=False).mean()\n",
    "                df[f'{col}_ema_{span}h_std'] = df[col].ewm(span=span, adjust=False).std().fillna(0)\n",
    "\n",
    "    # Fill NaN values created by lag features\n",
    "    df = df.bfill().ffill().fillna(0)\n",
    "\n",
    "print(f\" Feature engineering complete. Total features: {len(df.columns)}\")\n",
    "\n",
    "# Prepare features and target\n",
    "target_col = 'Calculated_AQI'  # Focus on this since backend uses it\n",
    "if target_col not in df.columns:\n",
    "    target_col = 'aqi_index'\n",
    "    print(f\"   ️  WARNING: Calculated_AQI not found, using aqi_index instead\")\n",
    "    print(f\"   ️  WARNING: This will cause predictions to be in wrong scale (3-5 instead of 70-500)\")\n",
    "    print(f\"   ️  WARNING: Backend will need to convert predictions to actual AQI scale\")\n",
    "\n",
    "# Drop target and datetime columns\n",
    "# IMPORTANT: Exclude target and any related columns to prevent data leakage\n",
    "exclude_cols = [\n",
    "    target_col, 'aqi_index', 'datetime', 'event_timestamp', 'timestamp',\n",
    "    'created', 'index'\n",
    "]\n",
    "\n",
    "# Also exclude any lag features of the target (if they exist from previous runs)\n",
    "# CRITICAL: Never use target variable or its lags as features (data leakage!)\n",
    "target_lag_features = [col for col in df.columns if f'{target_col}_lag' in col.lower() or 'calculated_aqi_lag' in col.lower() or 'aqi_index_lag' in col.lower()]\n",
    "if target_lag_features:\n",
    "    print(f\"   ️  Found {len(target_lag_features)} target lag features - EXCLUDING to prevent data leakage\")\n",
    "    print(f\"      Examples: {target_lag_features[:3]}\")\n",
    "exclude_cols.extend(target_lag_features)\n",
    "\n",
    "# Double-check: remove any columns that contain the target name (except exact match which is already excluded)\n",
    "target_related = [col for col in df.columns if target_col.lower() in col.lower() and col != target_col]\n",
    "if target_related:\n",
    "    print(f\"   ️  Found {len(target_related)} target-related features - EXCLUDING\")\n",
    "    exclude_cols.extend(target_related)\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# If using minimal features, only keep raw pollutants and weather\n",
    "if USE_MINIMAL_FEATURES:\n",
    "    minimal_cols = pollutant_cols + weather_cols + ['year', 'month', 'day', 'hour']\n",
    "    feature_cols = [col for col in feature_cols if col in minimal_cols]\n",
    "    print(f\"   Using minimal feature set: {len(feature_cols)} features\")\n",
    "    print(f\"     Features: {', '.join(feature_cols[:10])}{'...' if len(feature_cols) > 10 else ''}\")\n",
    "\n",
    "# Select only numeric features\n",
    "X = df[feature_cols].select_dtypes(include=[np.number])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"\\n Model Configuration:\")\n",
    "print(f\"   Features: {len(X.columns)}\")\n",
    "print(f\"   Samples: {len(X)}\")\n",
    "print(f\"   Target: {target_col}\")\n",
    "print(f\"   Target range: {y.min():.2f} - {y.max():.2f}\")\n",
    "print(f\"   Target mean: {y.mean():.2f}\")\n",
    "print(f\"   Target std: {y.std():.2f}\")\n",
    "\n",
    "# Check feature-target correlations\n",
    "print(f\"\\n Feature Analysis:\")\n",
    "if len(X.columns) > 0:\n",
    "    # Calculate correlations with target\n",
    "    correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "    print(f\"   Top 10 most correlated features:\")\n",
    "    for feat, corr in correlations.head(10).items():\n",
    "        print(f\"      {feat}: {corr:.4f}\")\n",
    "    print(f\"   Features with correlation > 0.1: {(correlations > 0.1).sum()}\")\n",
    "    print(f\"   Features with correlation > 0.3: {(correlations > 0.3).sum()}\")\n",
    "\n",
    "# CRITICAL: Test if we can compute AQI directly from pollutants\n",
    "print(f\"\\n Testing Direct AQI Computation:\")\n",
    "print(\"   (Calculated_AQI should be MAX of individual pollutant AQIs)\")\n",
    "if all(col in df.columns for col in ['pm2_5', 'pm10', 'co', 'no2', 'o3', 'so2', 'nh3']):\n",
    "    # PM2.5 breakpoints (US EPA standard)\n",
    "    pm25_bp = [(0.0, 12.0, 0, 50), (12.1, 35.4, 51, 100), (35.5, 55.4, 101, 150),\n",
    "               (55.5, 150.4, 151, 200), (150.5, 250.4, 201, 300), (250.5, 350.4, 301, 400),\n",
    "               (350.5, 500.4, 401, 500)]\n",
    "    \n",
    "    def calc_aqi(conc, breakpoints):\n",
    "        for c_low, c_high, i_low, i_high in breakpoints:\n",
    "            if c_low <= conc <= c_high:\n",
    "                return ((i_high - i_low) / (c_high - c_low)) * (conc - c_low) + i_low\n",
    "        return None\n",
    "    \n",
    "    # Compute AQI from pollutants (simplified - using PM2.5 breakpoints for all)\n",
    "    computed_aqi = []\n",
    "    for idx, row in df.head(100).iterrows():  # Test first 100 rows\n",
    "        aqis = []\n",
    "        for pol in ['pm2_5', 'pm10', 'co', 'no2', 'o3', 'so2', 'nh3']:\n",
    "            if pol in row and not pd.isna(row[pol]) and row[pol] >= 0:\n",
    "                aqi_val = calc_aqi(row[pol], pm25_bp)\n",
    "                if aqi_val is not None:\n",
    "                    aqis.append(aqi_val)\n",
    "        computed_aqi.append(max(aqis) if aqis else None)\n",
    "    \n",
    "    # Compare with actual Calculated_AQI\n",
    "    actual_aqi = df.head(100)[target_col].values\n",
    "    valid_mask = ~pd.isna(computed_aqi) & ~pd.isna(actual_aqi)\n",
    "    if valid_mask.sum() > 0:\n",
    "        computed_arr = np.array(computed_aqi)[valid_mask]\n",
    "        actual_arr = actual_aqi[valid_mask]\n",
    "        match_rate = (np.abs(computed_arr - actual_arr) < 1).sum() / len(computed_arr) * 100\n",
    "        print(f\"   Computed vs Actual AQI match rate: {match_rate:.1f}% (within 1 unit)\")\n",
    "        print(f\"   Mean difference: {np.mean(np.abs(computed_arr - actual_arr)):.2f}\")\n",
    "        if match_rate < 50:\n",
    "            print(f\"   ️  WARNING: Low match rate suggests Calculated_AQI may not be computed correctly!\")\n",
    "    else:\n",
    "        print(f\"   ️  Could not compute AQI from pollutants - check data quality\")\n",
    "else:\n",
    "    print(f\"   ️  Missing pollutant columns - cannot test direct computation\")\n",
    "\n",
    "# Remove any remaining NaN or inf\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "y = y.replace([np.inf, -np.inf], np.nan).fillna(y.mean())\n",
    "\n",
    "# Feature selection - remove highly correlated features (BEFORE split to avoid data leakage)\n",
    "print(\"\\n Feature selection...\")\n",
    "correlation_threshold = 0.95\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "if high_corr_features:\n",
    "    print(f\"   Removing {len(high_corr_features)} highly correlated features\")\n",
    "    X = X.drop(columns=high_corr_features)\n",
    "\n",
    "# Remove features with very low variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "variance_selector = VarianceThreshold(threshold=0.01)\n",
    "X_array = variance_selector.fit_transform(X)\n",
    "selected_features = X.columns[variance_selector.get_support()]\n",
    "X = pd.DataFrame(X_array, columns=selected_features, index=X.index)\n",
    "print(f\"   Final feature count: {len(X.columns)}\")\n",
    "\n",
    "# Check if data is sorted by target OR if first portion has zero variance (CRITICAL ISSUE!)\n",
    "print(\"\\n Checking data order and variance...\")\n",
    "is_sorted_inc = y.is_monotonic_increasing\n",
    "is_sorted_dec = y.is_monotonic_decreasing\n",
    "is_sorted = is_sorted_inc or is_sorted_dec\n",
    "\n",
    "# Check variance in first 70% (what will become training set)\n",
    "train_size_check = int(len(y) * 0.7)\n",
    "first_portion_std = y.iloc[:train_size_check].std()\n",
    "first_portion_unique = y.iloc[:train_size_check].nunique()\n",
    "\n",
    "print(f\"   First 70% of data: std={first_portion_std:.2f}, unique values={first_portion_unique}\")\n",
    "print(f\"   Full dataset: std={y.std():.2f}, unique values={y.nunique()}\")\n",
    "\n",
    "if is_sorted:\n",
    "    print(f\"   ️  CRITICAL: Data appears to be sorted by target value!\")\n",
    "    print(f\"   Sorted {'increasing' if is_sorted_inc else 'decreasing'}\")\n",
    "    print(f\"   This will cause all splits to have the same values!\")\n",
    "    print(f\"   Solution: Shuffling data before split (temporal order may be lost)\")\n",
    "    shuffle_needed = True\n",
    "elif first_portion_std == 0 or first_portion_unique == 1:\n",
    "    print(f\"   ️  CRITICAL: First 70% of data has ZERO variance!\")\n",
    "    print(f\"   All training samples would have the same target value!\")\n",
    "    print(f\"   Solution: Shuffling data before split\")\n",
    "    shuffle_needed = True\n",
    "else:\n",
    "    print(f\"    Data appears OK - first portion has variance\")\n",
    "    shuffle_needed = False\n",
    "\n",
    "if shuffle_needed:\n",
    "    # Shuffle the data to break the pattern\n",
    "    shuffle_indices = np.random.RandomState(42).permutation(len(X))\n",
    "    X = X.iloc[shuffle_indices].reset_index(drop=True)\n",
    "    y = y.iloc[shuffle_indices].reset_index(drop=True)\n",
    "    print(f\"    Data shuffled to break pattern\")\n",
    "    # Verify shuffle worked\n",
    "    new_first_portion_std = y.iloc[:train_size_check].std()\n",
    "    print(f\"   After shuffle - First 70% std: {new_first_portion_std:.2f}\")\n",
    "\n",
    "# Time series split (preserve temporal order) with validation set\n",
    "# IMPORTANT: Split BEFORE outlier removal to preserve distribution\n",
    "print(\"\\n Splitting data...\")\n",
    "train_idx = int(len(X) * 0.7)  # 70% for training\n",
    "val_idx = int(len(X) * 0.85)   # 15% for validation\n",
    "X_train = X.iloc[:train_idx].copy()\n",
    "X_val = X.iloc[train_idx:val_idx].copy()\n",
    "X_test = X.iloc[val_idx:].copy()\n",
    "y_train = y.iloc[:train_idx].copy()\n",
    "y_val = y.iloc[train_idx:val_idx].copy()\n",
    "y_test = y.iloc[val_idx:].copy()\n",
    "\n",
    "# Remove outliers from target (cap at 3 standard deviations) - AFTER split\n",
    "# Use training set statistics to avoid data leakage\n",
    "print(\"\\n Removing outliers (using training statistics)...\")\n",
    "y_train_mean = y_train.mean()\n",
    "y_train_std = y_train.std()\n",
    "\n",
    "if y_train_std == 0:\n",
    "    print(f\"   ️  CRITICAL: Training set has ZERO variance before outlier removal!\")\n",
    "    print(f\"   All training samples have the same target value: {y_train_mean:.2f}\")\n",
    "    print(f\"   Skipping outlier removal - would remove all data!\")\n",
    "    print(f\"   This suggests data was sorted - check if shuffle worked\")\n",
    "else:\n",
    "    # Use a more lenient threshold (5 std devs instead of 3) to avoid removing too much\n",
    "    outlier_threshold = 5 * y_train_std  # More lenient\n",
    "    y_lower = max(0, y_train_mean - outlier_threshold)  # AQI can't be negative\n",
    "    y_upper = y_train_mean + outlier_threshold\n",
    "    \n",
    "    # Apply same threshold to all sets\n",
    "    train_mask = (y_train >= y_lower) & (y_train <= y_upper)\n",
    "    val_mask = (y_val >= y_lower) & (y_val <= y_upper)\n",
    "    test_mask = (y_test >= y_lower) & (y_test <= y_upper)\n",
    "    \n",
    "    # Only remove if we keep at least 90% of data\n",
    "    train_keep_ratio = train_mask.sum() / len(train_mask)\n",
    "    if train_keep_ratio < 0.9:\n",
    "        print(f\"   ️  Outlier removal would remove {100*(1-train_keep_ratio):.1f}% of training data\")\n",
    "        print(f\"   Skipping outlier removal to preserve data\")\n",
    "    else:\n",
    "        X_train = X_train[train_mask].copy()\n",
    "        y_train = y_train[train_mask].copy()\n",
    "        X_val = X_val[val_mask].copy()\n",
    "        y_val = y_val[val_mask].copy()\n",
    "        X_test = X_test[test_mask].copy()\n",
    "        y_test = y_test[test_mask].copy()\n",
    "        \n",
    "        print(f\"   Removed {len(train_mask) - train_mask.sum()} train outliers ({100*(1-train_keep_ratio):.1f}%)\")\n",
    "        print(f\"   Removed {len(val_mask) - val_mask.sum()} val outliers\")\n",
    "        print(f\"   Removed {len(test_mask) - test_mask.sum()} test outliers\")\n",
    "        print(f\"   Train target range: {y_train.min():.2f} - {y_train.max():.2f}\")\n",
    "        print(f\"   Test target range: {y_test.min():.2f} - {y_test.max():.2f}\")\n",
    "\n",
    "print(f\"   Train: {len(X_train)} samples\")\n",
    "print(f\"   Validation: {len(X_val)} samples\")\n",
    "print(f\"   Test: {len(X_test)} samples\")\n",
    "\n",
    "# Check for distribution shift\n",
    "print(f\"\\n Distribution Check:\")\n",
    "print(f\"   Train target - mean: {y_train.mean():.2f}, std: {y_train.std():.2f}, min: {y_train.min():.2f}, max: {y_train.max():.2f}\")\n",
    "print(f\"   Val target - mean: {y_val.mean():.2f}, std: {y_val.std():.2f}, min: {y_val.min():.2f}, max: {y_val.max():.2f}\")\n",
    "print(f\"   Test target - mean: {y_test.mean():.2f}, std: {y_test.std():.2f}, min: {y_test.min():.2f}, max: {y_test.max():.2f}\")\n",
    "\n",
    "# Check if training set has zero variance (critical issue!)\n",
    "if y_train.std() == 0 or y_train.nunique() == 1:\n",
    "    print(f\"   ️  CRITICAL: Training set has ZERO variance after split!\")\n",
    "    print(f\"   All training samples have the same target value: {y_train.mean():.2f}\")\n",
    "    print(f\"   This will cause all models to predict the same value.\")\n",
    "    print(f\"   FIXING: Shuffling data and re-splitting...\")\n",
    "    \n",
    "    # Emergency fix: shuffle and re-split\n",
    "    combined_indices = np.random.RandomState(42).permutation(len(X))\n",
    "    X_combined = X.iloc[combined_indices].reset_index(drop=True)\n",
    "    y_combined = y.iloc[combined_indices].reset_index(drop=True)\n",
    "    \n",
    "    train_idx = int(len(X_combined) * 0.7)\n",
    "    val_idx = int(len(X_combined) * 0.85)\n",
    "    X_train = X_combined.iloc[:train_idx].copy()\n",
    "    X_val = X_combined.iloc[train_idx:val_idx].copy()\n",
    "    X_test = X_combined.iloc[val_idx:].copy()\n",
    "    y_train = y_combined.iloc[:train_idx].copy()\n",
    "    y_val = y_combined.iloc[train_idx:val_idx].copy()\n",
    "    y_test = y_combined.iloc[val_idx:].copy()\n",
    "    \n",
    "    print(f\"    Re-split after shuffle\")\n",
    "    print(f\"   Train target - mean: {y_train.mean():.2f}, std: {y_train.std():.2f}, unique: {y_train.nunique()}\")\n",
    "    print(f\"   Test target - mean: {y_test.mean():.2f}, std: {y_test.std():.2f}, unique: {y_test.nunique()}\")\n",
    "    \n",
    "    if y_train.std() == 0:\n",
    "        print(f\"   ️  STILL ZERO VARIANCE after shuffle!\")\n",
    "        print(f\"   This suggests a fundamental data issue - all Calculated_AQI values may be identical\")\n",
    "        print(f\"   Cannot train meaningful model - exiting\")\n",
    "        raise ValueError(\"Training set has zero variance even after shuffling. Check data quality.\")\n",
    "else:\n",
    "    mean_shift = abs(y_train.mean() - y_test.mean()) / y_train.std() if y_train.std() > 0 else 0\n",
    "    std_shift = abs(y_train.std() - y_test.std()) / y_train.std() if y_train.std() > 0 else 0\n",
    "    if mean_shift > 0.2:\n",
    "        print(f\"   ️  WARNING: Significant mean shift between train/test: {mean_shift:.2f} std devs\")\n",
    "    if std_shift > 0.2:\n",
    "        print(f\"   ️  WARNING: Significant std shift between train/test: {std_shift:.2f} std devs\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train models\n",
    "print(\"\\n Training models...\")\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "# First, try a simple baseline model to establish if features are predictive\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "print(\"\\n  Training Simple Baseline Models...\")\n",
    "try:\n",
    "    # Simple Linear Regression\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "    y_pred_lr = np.clip(y_pred_lr, 0, None)\n",
    "    \n",
    "    lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "    lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "    lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "    \n",
    "    baseline_rmse = np.sqrt(mean_squared_error(y_test, np.full_like(y_test, y_test.mean())))\n",
    "    print(f\"    Linear Regression - RMSE: {lr_rmse:.4f}, MAE: {lr_mae:.4f}, R²: {lr_r2:.4f}\")\n",
    "    print(f\"    Baseline (mean) - RMSE: {baseline_rmse:.4f}\")\n",
    "    if lr_r2 < 0:\n",
    "        print(f\"    ️  WARNING: Even simple linear regression has negative R²!\")\n",
    "        print(f\"    This suggests features may not be predictive or there's a fundamental issue.\")\n",
    "    \n",
    "    # Ridge Regression (regularized)\n",
    "    ridge_model = Ridge(alpha=1.0)\n",
    "    ridge_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "    y_pred_ridge = np.clip(y_pred_ridge, 0, None)\n",
    "    \n",
    "    ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "    ridge_mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "    ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
    "    print(f\"    Ridge Regression - RMSE: {ridge_rmse:.4f}, MAE: {ridge_mae:.4f}, R²: {ridge_r2:.4f}\")\n",
    "    \n",
    "    models['LinearRegression'] = lr_model\n",
    "    results.append({\n",
    "        'Model': 'LinearRegression',\n",
    "        'RMSE': lr_rmse,\n",
    "        'MAE': lr_mae,\n",
    "        'R²': lr_r2\n",
    "    })\n",
    "    \n",
    "    models['Ridge'] = ridge_model\n",
    "    results.append({\n",
    "        'Model': 'Ridge',\n",
    "        'RMSE': ridge_rmse,\n",
    "        'MAE': ridge_mae,\n",
    "        'R²': ridge_r2\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(f\"    ️  Baseline model training failed: {e}\")\n",
    "\n",
    "# XGBoost with improved hyperparameters (reduced overfitting)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"\\n  Training XGBoost (with regularization)...\")\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=5,  # Reduced depth to prevent overfitting\n",
    "        learning_rate=0.01,  # Slightly higher learning rate\n",
    "        subsample=0.8,  # More subsampling for regularization\n",
    "        colsample_bytree=0.8,  # More feature subsampling\n",
    "        colsample_bylevel=0.8,\n",
    "        colsample_bynode=0.8,\n",
    "        min_child_weight=5,  # Increased to prevent overfitting\n",
    "        gamma=0.2,  # Increased minimum loss reduction\n",
    "        reg_alpha=0.5,  # Increased L1 regularization\n",
    "        reg_lambda=2.0,  # Increased L2 regularization\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=100,  # Earlier stopping\n",
    "        eval_metric='rmse',\n",
    "        tree_method='hist',\n",
    "        grow_policy='lossguide'\n",
    "    )\n",
    "    xgb_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_val_scaled, y_val), (X_test_scaled, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Clip predictions to reasonable range\n",
    "    y_pred_xgb = np.clip(y_pred_xgb, 0, None)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "    mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    r2 = r2_score(y_test, y_pred_xgb)\n",
    "    \n",
    "    # Comprehensive diagnostic information\n",
    "    baseline_rmse = np.sqrt(mean_squared_error(y_test, np.full_like(y_test, y_test.mean())))\n",
    "    baseline_mae = mean_absolute_error(y_test, np.full_like(y_test, y_test.mean()))\n",
    "    \n",
    "    print(f\"    RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    print(f\"    Baseline RMSE (mean): {baseline_rmse:.4f}\")\n",
    "    print(f\"    Baseline MAE (mean): {baseline_mae:.4f}\")\n",
    "    print(f\"    Improvement over baseline RMSE: {((baseline_rmse - rmse) / baseline_rmse * 100):.2f}%\")\n",
    "    print(f\"    Improvement over baseline MAE: {((baseline_mae - mae) / baseline_mae * 100):.2f}%\")\n",
    "    print(f\"    Test target - mean: {y_test.mean():.2f}, std: {y_test.std():.2f}, min: {y_test.min():.2f}, max: {y_test.max():.2f}\")\n",
    "    print(f\"    Predictions - mean: {y_pred_xgb.mean():.2f}, std: {y_pred_xgb.std():.2f}, min: {y_pred_xgb.min():.2f}, max: {y_pred_xgb.max():.2f}\")\n",
    "    \n",
    "    # Check prediction distribution\n",
    "    pred_error = y_test - y_pred_xgb\n",
    "    print(f\"    Prediction errors - mean: {pred_error.mean():.2f}, std: {pred_error.std():.2f}\")\n",
    "    print(f\"    Mean absolute error: {np.abs(pred_error).mean():.2f}\")\n",
    "    \n",
    "    # Check if model is systematically biased\n",
    "    if abs(pred_error.mean()) > y_test.std() * 0.1:\n",
    "        print(f\"    ️  WARNING: Model has systematic bias of {pred_error.mean():.2f}\")\n",
    "    \n",
    "    models['XGBoost'] = xgb_model\n",
    "    results.append({\n",
    "        'Model': 'XGBoost',\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2\n",
    "    })\n",
    "\n",
    "# LightGBM with improved hyperparameters (reduced overfitting)\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    print(\"\\n  Training LightGBM (with regularization)...\")\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=5,  # Reduced depth\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.8,  # More subsampling\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_samples=30,  # Increased\n",
    "        reg_alpha=0.5,  # Increased L1 regularization\n",
    "        reg_lambda=2.0,  # Increased L2 regularization\n",
    "        num_leaves=31,  # Reduced leaves\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        min_data_in_leaf=20,  # Increased\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "        boosting_type='gbdt',\n",
    "        objective='regression',\n",
    "        metric='rmse'\n",
    "    )\n",
    "    lgb_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_val_scaled, y_val), (X_test_scaled, y_test)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    y_pred_lgb = lgb_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Clip predictions to reasonable range\n",
    "    y_pred_lgb = np.clip(y_pred_lgb, 0, None)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\n",
    "    mae = mean_absolute_error(y_test, y_pred_lgb)\n",
    "    r2 = r2_score(y_test, y_pred_lgb)\n",
    "    \n",
    "    models['LightGBM'] = lgb_model\n",
    "    results.append({\n",
    "        'Model': 'LightGBM',\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2\n",
    "    })\n",
    "    print(f\"    RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# Gradient Boosting (fallback) with improved hyperparameters\n",
    "print(\"\\n  Training Gradient Boosting (with regularization)...\")\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,  # Reduced depth\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,  # More subsampling\n",
    "    min_samples_split=30,  # Increased\n",
    "    min_samples_leaf=15,  # Increased\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=50,  # Earlier stopping\n",
    "    loss='squared_error',\n",
    "    criterion='friedman_mse'\n",
    ")\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Clip predictions to reasonable range\n",
    "y_pred_gb = np.clip(y_pred_gb, 0, None)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "mae = mean_absolute_error(y_test, y_pred_gb)\n",
    "r2 = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "models['GradientBoosting'] = gb_model\n",
    "results.append({\n",
    "    'Model': 'GradientBoosting',\n",
    "    'RMSE': rmse,\n",
    "    'MAE': mae,\n",
    "    'R²': r2\n",
    "})\n",
    "print(f\"    RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "# LSTM Model with improved architecture and target scaling\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n  Training LSTM...\")\n",
    "    try:\n",
    "        # Prepare sequence data for LSTM\n",
    "        sequence_length = 24  # Look back 24 hours\n",
    "        forecast_horizon = 1   # Predict 1 hour ahead\n",
    "        \n",
    "        # Use MinMaxScaler for features (better for neural networks)\n",
    "        lstm_scaler = MinMaxScaler()\n",
    "        X_train_lstm_scaled = lstm_scaler.fit_transform(X_train)\n",
    "        X_val_lstm_scaled = lstm_scaler.transform(X_val)\n",
    "        X_test_lstm_scaled = lstm_scaler.transform(X_test)\n",
    "        \n",
    "        # Scale target separately for better learning\n",
    "        y_scaler = MinMaxScaler()\n",
    "        y_train_scaled = y_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "        y_val_scaled = y_scaler.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "        y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Create sequences\n",
    "        X_train_seq, y_train_seq = create_sequences(\n",
    "            X_train_lstm_scaled, y_train_scaled, \n",
    "            sequence_length=sequence_length, \n",
    "            forecast_horizon=forecast_horizon\n",
    "        )\n",
    "        X_val_seq, y_val_seq = create_sequences(\n",
    "            X_val_lstm_scaled, y_val_scaled,\n",
    "            sequence_length=sequence_length,\n",
    "            forecast_horizon=forecast_horizon\n",
    "        )\n",
    "        X_test_seq, y_test_seq = create_sequences(\n",
    "            X_test_lstm_scaled, y_test_scaled,\n",
    "            sequence_length=sequence_length,\n",
    "            forecast_horizon=forecast_horizon\n",
    "        )\n",
    "        \n",
    "        # Reshape y for single output\n",
    "        if y_train_seq.ndim > 1:\n",
    "            y_train_seq = y_train_seq.reshape(-1)\n",
    "        if y_val_seq.ndim > 1:\n",
    "            y_val_seq = y_val_seq.reshape(-1)\n",
    "        if y_test_seq.ndim > 1:\n",
    "            y_test_seq = y_test_seq.reshape(-1)\n",
    "        \n",
    "        print(f\"    Sequence shape: {X_train_seq.shape}\")\n",
    "        print(f\"    Training sequences: {len(X_train_seq)}\")\n",
    "        \n",
    "        # Build improved LSTM model with regularization\n",
    "        lstm_model = Sequential([\n",
    "            LSTM(256, return_sequences=True, input_shape=(sequence_length, X_train_seq.shape[2])),\n",
    "            Dropout(0.3),\n",
    "            BatchNormalization(),\n",
    "            LSTM(128, return_sequences=True),\n",
    "            Dropout(0.3),\n",
    "            BatchNormalization(),\n",
    "            LSTM(64, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        lstm_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=0),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-8, verbose=0),\n",
    "        ]\n",
    "        \n",
    "        # Train model with validation set\n",
    "        history = lstm_model.fit(\n",
    "            X_train_seq, y_train_seq,\n",
    "            validation_data=(X_val_seq, y_val_seq),\n",
    "            epochs=200,\n",
    "            batch_size=128,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Predict and inverse transform\n",
    "        y_pred_lstm_scaled = lstm_model.predict(X_test_seq, verbose=0).reshape(-1)\n",
    "        y_pred_lstm = y_scaler.inverse_transform(y_pred_lstm_scaled.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Clip predictions to reasonable range\n",
    "        y_pred_lstm = np.clip(y_pred_lstm, 0, None)\n",
    "        \n",
    "        # Get actual y values (not scaled) for comparison\n",
    "        y_test_actual = y_test.iloc[sequence_length:].values\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred_lstm))\n",
    "        mae = mean_absolute_error(y_test_actual, y_pred_lstm)\n",
    "        r2 = r2_score(y_test_actual, y_pred_lstm)\n",
    "        \n",
    "        models['LSTM'] = {\n",
    "            'model': lstm_model,\n",
    "            'scaler': lstm_scaler,\n",
    "            'y_scaler': y_scaler,\n",
    "            'sequence_length': sequence_length\n",
    "        }\n",
    "        results.append({\n",
    "            'Model': 'LSTM',\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2\n",
    "        })\n",
    "        print(f\"    RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ️ LSTM training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# GRU Model with improved architecture and target scaling\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    print(\"\\n  Training GRU...\")\n",
    "    try:\n",
    "        # Prepare sequence data for GRU\n",
    "        sequence_length = 24\n",
    "        forecast_horizon = 1\n",
    "        \n",
    "        # Use MinMaxScaler for features\n",
    "        gru_scaler = MinMaxScaler()\n",
    "        X_train_gru_scaled = gru_scaler.fit_transform(X_train)\n",
    "        X_val_gru_scaled = gru_scaler.transform(X_val)\n",
    "        X_test_gru_scaled = gru_scaler.transform(X_test)\n",
    "        \n",
    "        # Scale target separately\n",
    "        y_scaler_gru = MinMaxScaler()\n",
    "        y_train_scaled = y_scaler_gru.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "        y_val_scaled = y_scaler_gru.transform(y_val.values.reshape(-1, 1)).ravel()\n",
    "        y_test_scaled = y_scaler_gru.transform(y_test.values.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Create sequences\n",
    "        X_train_seq, y_train_seq = create_sequences(\n",
    "            X_train_gru_scaled, y_train_scaled,\n",
    "            sequence_length=sequence_length,\n",
    "            forecast_horizon=forecast_horizon\n",
    "        )\n",
    "        X_val_seq, y_val_seq = create_sequences(\n",
    "            X_val_gru_scaled, y_val_scaled,\n",
    "            sequence_length=sequence_length,\n",
    "            forecast_horizon=forecast_horizon\n",
    "        )\n",
    "        X_test_seq, y_test_seq = create_sequences(\n",
    "            X_test_gru_scaled, y_test_scaled,\n",
    "            sequence_length=sequence_length,\n",
    "            forecast_horizon=forecast_horizon\n",
    "        )\n",
    "        \n",
    "        # Reshape y for single output\n",
    "        if y_train_seq.ndim > 1:\n",
    "            y_train_seq = y_train_seq.reshape(-1)\n",
    "        if y_val_seq.ndim > 1:\n",
    "            y_val_seq = y_val_seq.reshape(-1)\n",
    "        if y_test_seq.ndim > 1:\n",
    "            y_test_seq = y_test_seq.reshape(-1)\n",
    "        \n",
    "        print(f\"    Sequence shape: {X_train_seq.shape}\")\n",
    "        print(f\"    Training sequences: {len(X_train_seq)}\")\n",
    "        \n",
    "        # Build improved GRU model with regularization\n",
    "        gru_model = Sequential([\n",
    "            GRU(256, return_sequences=True, input_shape=(sequence_length, X_train_seq.shape[2]),\n",
    "                dropout=0.3, recurrent_dropout=0.3),\n",
    "            BatchNormalization(),\n",
    "            GRU(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
    "            BatchNormalization(),\n",
    "            GRU(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        \n",
    "        gru_model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mse',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=0),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-8, verbose=0),\n",
    "        ]\n",
    "        \n",
    "        # Train model with validation set\n",
    "        history = gru_model.fit(\n",
    "            X_train_seq, y_train_seq,\n",
    "            validation_data=(X_val_seq, y_val_seq),\n",
    "            epochs=200,\n",
    "            batch_size=128,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Predict and inverse transform\n",
    "        y_pred_gru_scaled = gru_model.predict(X_test_seq, verbose=0).reshape(-1)\n",
    "        y_pred_gru = y_scaler_gru.inverse_transform(y_pred_gru_scaled.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # Clip predictions to reasonable range\n",
    "        y_pred_gru = np.clip(y_pred_gru, 0, None)\n",
    "        \n",
    "        # Get actual y values (not scaled) for comparison\n",
    "        y_test_actual = y_test.iloc[sequence_length:].values\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test_actual, y_pred_gru))\n",
    "        mae = mean_absolute_error(y_test_actual, y_pred_gru)\n",
    "        r2 = r2_score(y_test_actual, y_pred_gru)\n",
    "        \n",
    "        models['GRU'] = {\n",
    "            'model': gru_model,\n",
    "            'scaler': gru_scaler,\n",
    "            'y_scaler': y_scaler_gru,\n",
    "            'sequence_length': sequence_length\n",
    "        }\n",
    "        results.append({\n",
    "            'Model': 'GRU',\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2\n",
    "        })\n",
    "        print(f\"    RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ️ GRU training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Select best model (before ensemble to get RMSE values)\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Ensemble Model - combine top models\n",
    "print(\"\\n  Training Ensemble Model...\")\n",
    "try:\n",
    "    # Get predictions from all tree-based models\n",
    "    ensemble_predictions = []\n",
    "    ensemble_weights = []\n",
    "    ensemble_model_names = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        if model_name not in ['LSTM', 'GRU']:  # Only use tree-based models for ensemble\n",
    "            try:\n",
    "                # Use validation set for weighting to avoid overfitting\n",
    "                pred_val = model.predict(X_val_scaled)\n",
    "                pred_val = np.clip(pred_val, 0, None)\n",
    "                val_rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n",
    "                \n",
    "                pred = model.predict(X_test_scaled)\n",
    "                pred = np.clip(pred, 0, None)\n",
    "                ensemble_predictions.append(pred)\n",
    "                ensemble_model_names.append(model_name)\n",
    "                # Weight by inverse validation RMSE (better for generalization)\n",
    "                ensemble_weights.append(1.0 / (val_rmse + 1e-6))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if ensemble_predictions and len(ensemble_predictions) > 1:\n",
    "        # Normalize weights\n",
    "        ensemble_weights = np.array(ensemble_weights)\n",
    "        ensemble_weights = ensemble_weights / ensemble_weights.sum()\n",
    "        \n",
    "        # Weighted average\n",
    "        y_pred_ensemble = np.zeros_like(ensemble_predictions[0])\n",
    "        for pred, weight in zip(ensemble_predictions, ensemble_weights):\n",
    "            y_pred_ensemble += pred * weight\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred_ensemble))\n",
    "        mae = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "        r2 = r2_score(y_test, y_pred_ensemble)\n",
    "        \n",
    "        models['Ensemble'] = {\n",
    "            'models': {k: v for k, v in models.items() if k in ensemble_model_names},\n",
    "            'weights': ensemble_weights.tolist(),\n",
    "            'model_names': ensemble_model_names\n",
    "        }\n",
    "        results.append({\n",
    "            'Model': 'Ensemble',\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R²': r2\n",
    "        })\n",
    "        print(f\"    RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "    else:\n",
    "        print(\"    ️ Not enough models for ensemble\")\n",
    "except Exception as e:\n",
    "    print(f\"    ️ Ensemble training failed: {e}\")\n",
    "\n",
    "# Update results_df with ensemble\n",
    "results_df = pd.DataFrame(results)\n",
    "if len(results_df) == 0:\n",
    "    raise ValueError(\"No models were successfully trained! Check for errors above.\")\n",
    "\n",
    "best_model_name = results_df.loc[results_df['RMSE'].idxmin(), 'Model']\n",
    "if best_model_name not in models:\n",
    "    raise KeyError(f\"Best model '{best_model_name}' not found in models dictionary. Available models: {list(models.keys())}\")\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\n Best Model: {best_model_name}\")\n",
    "print(results_df)\n",
    "\n",
    "# Save model and scaler\n",
    "print(\"\\n Saving model and scaler...\")\n",
    "model_path = Path(\"best_model.pkl\")\n",
    "scaler_path = Path(\"scaler.pkl\")\n",
    "metadata_path = Path(\"best_model_metadata.json\")\n",
    "feature_names_path = Path(\"feature_names.json\")\n",
    "\n",
    "# Handle different model types\n",
    "if best_model_name in ['LSTM', 'GRU']:\n",
    "    # Save deep learning model\n",
    "    model_dir = Path(\"best_model_keras\")\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    best_model['model'].save(str(model_dir))\n",
    "    print(f\" Keras model saved to {model_dir}\")\n",
    "    \n",
    "    # Save scaler and metadata for deep learning model\n",
    "    joblib.dump(best_model['scaler'], scaler_path)\n",
    "    print(f\" Scaler saved to {scaler_path}\")\n",
    "    \n",
    "    # Save sequence length info\n",
    "    sequence_info = {\n",
    "        'sequence_length': best_model['sequence_length'],\n",
    "        'model_type': best_model_name\n",
    "    }\n",
    "    with open(Path(\"sequence_info.json\"), 'w') as f:\n",
    "        json.dump(sequence_info, f, indent=2)\n",
    "    print(f\" Sequence info saved to sequence_info.json\")\n",
    "elif best_model_name == 'Ensemble':\n",
    "    # Save ensemble model\n",
    "    ensemble_data = {\n",
    "        'model_names': best_model['model_names'],\n",
    "        'weights': best_model['weights']\n",
    "    }\n",
    "    # Save individual models\n",
    "    for model_name in best_model['model_names']:\n",
    "        model_path_individual = Path(f\"ensemble_{model_name.lower()}.pkl\")\n",
    "        joblib.dump(best_model['models'][model_name], model_path_individual)\n",
    "    \n",
    "    joblib.dump(ensemble_data, model_path)\n",
    "    print(f\" Ensemble model saved to {model_path}\")\n",
    "    \n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\" Scaler saved to {scaler_path}\")\n",
    "else:\n",
    "    # Save tree-based model\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\" Model saved to {model_path}\")\n",
    "    \n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\" Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"model_type\": best_model_name,\n",
    "    \"sklearn_version\": \"1.5.2\",\n",
    "    \"numpy_version\": np.__version__,\n",
    "    \"target_column\": target_col,\n",
    "    \"feature_count\": len(X.columns),\n",
    "    \"training_samples\": len(X_train),\n",
    "    \"validation_samples\": len(X_val),\n",
    "    \"test_samples\": len(X_test),\n",
    "    \"dataset_expanded\": True,\n",
    "    \"augmentation_factor\": 0 if not USE_AUGMENTATION else 2,  # No augmentation by default\n",
    "    \"train_val_test_split\": \"70/15/15\",\n",
    "    \"best_rmse\": float(results_df.loc[results_df['RMSE'].idxmin(), 'RMSE']),\n",
    "    \"best_mae\": float(results_df.loc[results_df['RMSE'].idxmin(), 'MAE']),\n",
    "    \"best_r2\": float(results_df.loc[results_df['RMSE'].idxmin(), 'R²']),\n",
    "    \"all_models\": results_df.to_dict('records'),\n",
    "    \"created_at\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Add TensorFlow version if available\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    metadata[\"tensorflow_version\"] = tf.__version__\n",
    "\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\" Metadata saved to {metadata_path}\")\n",
    "\n",
    "# Save feature names for prediction\n",
    "feature_names = {\n",
    "    \"feature_columns\": list(X.columns),\n",
    "    \"target_column\": target_col\n",
    "}\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    json.dump(feature_names, f, indent=2)\n",
    "print(f\" Feature names saved to {feature_names_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Test RMSE: {metadata['best_rmse']:.4f}\")\n",
    "print(f\"Test MAE: {metadata['best_mae']:.4f}\")\n",
    "print(f\"Test R²: {metadata['best_r2']:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
